# Causal Learning for Out-Of-Distribution (OOD) Generalization: A Comprehensive Literature Review
The report is available in the 'seminar.pdf' file, written in Persian.

# Abstract: 
Machine learning algorithms are usually built on the iid assumption that the training and test data are independent and identically distributed. This assumption means that the distribution of training data and test data are the same. In the real world, due to distributional shifts, this assumption is hardly fulfilled, which greatly reduces the accuracy of these classical machine learning algorithms. On the other hand, machine learning algorithms often use statistical models to model the dependence between data and labels, which aims to learn domain-independent representations. However, statistical models are superficial descriptions of reality because they only need to model dependencies rather than intrinsic causal mechanisms. When dependence changes with the target distribution, statistical models may fail to generalize. Causality, by focusing on the representation of structured knowledge about the data generation process that allows for interventions, can help to understand and address some of the limitations of current machine learning methods. Despite the success of statistical learning, these models provide a relatively superficial description of reality that holds only when the experimental conditions are invariant. Instead, the field of causal learning seeks to model the effect of distributional shifts with a combination of data-driven learning and assumptions not previously included in the statistical description of a system. For the problem caused by the distribution shift, where the distribution of the test data is different from the training data, the out-of-distribution generalization problem arises through which the algorithm can generalize well on the unseen test data. In this seminar, we will review the methods of the out-of-distribution generalization problem. We will also explain one of its methods called causality, which has recently received extensive attention, in more detail. 

Keywords: Representation Learning, Domain Generalization, Causal Inference, Out-Of-Distribution Generalization, Invariant Learning
